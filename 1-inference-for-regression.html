<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Inference for Regression | Statistical Inference via Data Science</title>
  <meta name="description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Inference for Regression | Statistical Inference via Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://moderndive.com/" />
  <meta property="og:image" content="https://moderndive.com/images/logos/book_cover.png" />
  <meta property="og:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="github-repo" content="moderndive/ModernDive_book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Inference for Regression | Statistical Inference via Data Science" />
  <meta name="twitter:site" content="@ModernDive" />
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="twitter:image" content="https://moderndive.com/images/logos/book_cover.png" />

<meta name="author" content="Chester Ismay and Albert Y. Kim   Foreword by Kelly S. McConville" />


<meta name="date" content="2021-01-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="images/logos/favicons/apple-touch-icon.png" />
  <link rel="shortcut icon" href="images/logos/favicons/favicon.ico" type="image/x-icon" />
<link rel="prev" href="foreword.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to ModernDive</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html"><i class="fa fa-check"></i><b>1</b> Inference for Regression</a><ul>
<li class="chapter" data-level="" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#inf-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="1.1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#regression-refresher"><i class="fa fa-check"></i><b>1.1</b> Regression refresher</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#teaching-evaluations-analysis"><i class="fa fa-check"></i><b>1.1.1</b> Teaching evaluations analysis</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#sampling-scenario"><i class="fa fa-check"></i><b>1.1.2</b> Sampling scenario</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#regression-interp"><i class="fa fa-check"></i><b>1.2</b> Interpreting regression tables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#regression-se"><i class="fa fa-check"></i><b>1.2.1</b> Standard error</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#regression-test-statistic"><i class="fa fa-check"></i><b>1.2.2</b> Test statistic</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#p-value"><i class="fa fa-check"></i><b>1.2.3</b> p-value</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>1.2.4</b> Confidence interval</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#regression-table-computation"><i class="fa fa-check"></i><b>1.2.5</b> How does R compute the table?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#regression-conditions"><i class="fa fa-check"></i><b>1.3</b> Conditions for inference for regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#residuals-refresher"><i class="fa fa-check"></i><b>1.3.1</b> Residuals refresher</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#linearity-of-relationship"><i class="fa fa-check"></i><b>1.3.2</b> Linearity of relationship</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#independence-of-residuals"><i class="fa fa-check"></i><b>1.3.3</b> Independence of residuals</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#normality-of-residuals"><i class="fa fa-check"></i><b>1.3.4</b> Normality of residuals</a></li>
<li class="chapter" data-level="1.3.5" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#equality-of-variance"><i class="fa fa-check"></i><b>1.3.5</b> Equality of variance</a></li>
<li class="chapter" data-level="1.3.6" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#what-is-the-conclusion"><i class="fa fa-check"></i><b>1.3.6</b> What’s the conclusion?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#infer-regression"><i class="fa fa-check"></i><b>1.4</b> Simulation-based inference for regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#confidence-interval-for-slope"><i class="fa fa-check"></i><b>1.4.1</b> Confidence interval for slope</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#hypothesis-test-for-slope"><i class="fa fa-check"></i><b>1.4.2</b> Hypothesis test for slope</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#inference-conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#theory-regression"><i class="fa fa-check"></i><b>1.5.1</b> Theory-based inference for regression</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#summary-of-statistical-inference"><i class="fa fa-check"></i><b>1.5.2</b> Summary of statistical inference</a></li>
<li class="chapter" data-level="1.5.3" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#additional-resources"><i class="fa fa-check"></i><b>1.5.3</b> Additional resources</a></li>
<li class="chapter" data-level="1.5.4" data-path="1-inference-for-regression.html"><a href="1-inference-for-regression.html#whats-to-come"><i class="fa fa-check"></i><b>1.5.4</b> What’s to come</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Inference via Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<html>
<img src='https://moderndive.com/wide_format.png' alt="ModernDive">
</html>
<div id="inference-for-regression" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Inference for Regression</h1>
<p>In our penultimate chapter, we’ll revisit the regression models we first studied in Chapters <a href="#regression"><strong>??</strong></a> and <a href="#multiple-regression"><strong>??</strong></a>. Armed with our knowledge of confidence intervals and hypothesis tests from Chapters <a href="#confidence-intervals"><strong>??</strong></a> and <a href="#hypothesis-testing"><strong>??</strong></a>, we’ll be able to apply statistical inference to further our understanding of relationships between outcome and explanatory variables.</p>
<div id="inf-packages" class="section level3 unnumbered">
<h3>Needed packages</h3>
<p>Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section <a href="#tidyverse-package"><strong>??</strong></a> that loading the <code>tidyverse</code> package by running <code>library(tidyverse)</code> loads the following commonly used data science packages all at once:</p>
<ul>
<li><code>ggplot2</code> for data visualization</li>
<li><code>dplyr</code> for data wrangling</li>
<li><code>tidyr</code> for converting data to “tidy” format</li>
<li><code>readr</code> for importing spreadsheet data into R</li>
<li>As well as the more advanced <code>purrr</code>, <code>tibble</code>, <code>stringr</code>, and <code>forcats</code> packages</li>
</ul>
<p>If needed, read Section <a href="#packages"><strong>??</strong></a> for information on how to install and load R packages.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="1-inference-for-regression.html#cb1-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="1-inference-for-regression.html#cb1-2"></a><span class="kw">library</span>(moderndive)</span>
<span id="cb1-3"><a href="1-inference-for-regression.html#cb1-3"></a><span class="kw">library</span>(infer)</span></code></pre></div>
</div>
<div id="regression-refresher" class="section level2">
<h2><span class="header-section-number">1.1</span> Regression refresher</h2>
<p>Before jumping into inference for regression, let’s remind ourselves of the University of Texas Austin teaching evaluations analysis in Section <a href="#model1"><strong>??</strong></a>.</p>
<div id="teaching-evaluations-analysis" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Teaching evaluations analysis</h3>
<p>Recall using simple linear regression  we modeled the relationship between</p>
<ol style="list-style-type: decimal">
<li>A numerical outcome variable <span class="math inline">\(y\)</span> (the instructor’s teaching score) and</li>
<li>A single numerical explanatory variable <span class="math inline">\(x\)</span> (the instructor’s “beauty” score).</li>
</ol>
<p>We first created an <code>evals_ch5</code> data frame that selected a subset of variables from the <code>evals</code> data frame included in the <code>moderndive</code> package. This <code>evals_ch5</code> data frame contains only the variables of interest for our analysis, in particular the instructor’s teaching <code>score</code> and the “beauty” rating <code>bty_avg</code>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="1-inference-for-regression.html#cb2-1"></a>evals_ch5 &lt;-<span class="st"> </span>evals <span class="op">%&gt;%</span></span>
<span id="cb2-2"><a href="1-inference-for-regression.html#cb2-2"></a><span class="st">  </span><span class="kw">select</span>(ID, score, bty_avg, age)</span>
<span id="cb2-3"><a href="1-inference-for-regression.html#cb2-3"></a><span class="kw">glimpse</span>(evals_ch5)</span></code></pre></div>
<pre><code>Rows: 463
Columns: 4
$ ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…
$ score   &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4…
$ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, 3.17, 3…
$ age     &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 4…</code></pre>
<p>In Subsection <a href="#model1EDA"><strong>??</strong></a>, we performed an exploratory data analysis of the relationship between these two variables of <code>score</code> and <code>bty_avg</code>. We saw there that a weakly positive correlation of 0.187 existed between the two variables.</p>
<p>This was evidenced in Figure <a href="1-inference-for-regression.html#fig:regline">1.1</a> of the scatterplot along with the “best-fitting” regression line that summarizes the linear relationship between the two variables of <code>score</code> and <code>bty_avg</code>. Recall in Subsection <a href="#leastsquares"><strong>??</strong></a> that we defined a “best-fitting” line as the line that minimizes the <em>sum of squared residuals</em>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="1-inference-for-regression.html#cb4-1"></a><span class="kw">ggplot</span>(evals_ch5, </span>
<span id="cb4-2"><a href="1-inference-for-regression.html#cb4-2"></a>       <span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span></span>
<span id="cb4-3"><a href="1-inference-for-regression.html#cb4-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb4-4"><a href="1-inference-for-regression.html#cb4-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Beauty Score&quot;</span>, </span>
<span id="cb4-5"><a href="1-inference-for-regression.html#cb4-5"></a>       <span class="dt">y =</span> <span class="st">&quot;Teaching Score&quot;</span>,</span>
<span id="cb4-6"><a href="1-inference-for-regression.html#cb4-6"></a>       <span class="dt">title =</span> <span class="st">&quot;Relationship between teaching and beauty scores&quot;</span>) <span class="op">+</span><span class="st">  </span></span>
<span id="cb4-7"><a href="1-inference-for-regression.html#cb4-7"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:regline"></span>
<img src="ModernDive_files/figure-html/regline-1.png" alt="Relationship with regression line." width="\textwidth" />
<p class="caption">
FIGURE 1.1: Relationship with regression line.
</p>
</div>
<p>Looking at this plot again, you might be asking, “Does that line really have all that positive of a slope?”. It does increase from left to right as the <code>bty_avg</code> variable increases, but by how much? To get to this information, recall that we followed a two-step procedure:</p>
<ol style="list-style-type: decimal">
<li>We first “fit” the linear regression model using the <code>lm()</code> function with the formula <code>score ~ bty_avg</code>. We saved this model in <code>score_model</code>.</li>
<li>We get the regression table by applying the <code>get_regression_table()</code> function from the <code>moderndive</code> package to <code>score_model</code>.</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="1-inference-for-regression.html#cb5-1"></a><span class="co"># Fit regression model:</span></span>
<span id="cb5-2"><a href="1-inference-for-regression.html#cb5-2"></a>score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch5)</span>
<span id="cb5-3"><a href="1-inference-for-regression.html#cb5-3"></a><span class="co"># Get regression table:</span></span>
<span id="cb5-4"><a href="1-inference-for-regression.html#cb5-4"></a><span class="kw">get_regression_table</span>(score_model)</span></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:regtable-11">TABLE 1.1: </span>Previously seen linear regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
3.880
</td>
<td style="text-align:right;">
0.076
</td>
<td style="text-align:right;">
50.96
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3.731
</td>
<td style="text-align:right;">
4.030
</td>
</tr>
<tr>
<td style="text-align:left;">
bty_avg
</td>
<td style="text-align:right;">
0.067
</td>
<td style="text-align:right;">
0.016
</td>
<td style="text-align:right;">
4.09
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.035
</td>
<td style="text-align:right;">
0.099
</td>
</tr>
</tbody>
</table>
<p>Using the values in the <code>estimate</code> column of the resulting regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>, we could then obtain the equation of the “best-fitting” regression line in Figure <a href="1-inference-for-regression.html#fig:regline">1.1</a>:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{y} &amp;= b_0 + b_1 \cdot x\\
\widehat{\text{score}} &amp;= b_0 + b_{\text{bty}\_\text{avg}} \cdot\text{bty}\_\text{avg}\\
&amp;= 3.880 + 0.067\cdot\text{bty}\_\text{avg}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(b_0\)</span> is the fitted intercept and <span class="math inline">\(b_1\)</span> is the fitted slope for <code>bty_avg</code>. Recall the interpretation of the <span class="math inline">\(b_1\)</span> = 0.067 value of the fitted slope:</p>
<blockquote>
<p>For every increase of one unit in “beauty” rating, there is an associated increase, on average, of 0.067 units of evaluation score.</p>
</blockquote>
<p>Thus, the slope value quantifies the relationship between the <span class="math inline">\(y\)</span> variable <code>score</code> and the <span class="math inline">\(x\)</span> variable <code>bty_avg</code>. We also discussed the intercept value of <span class="math inline">\(b_0\)</span> = 3.88 and its lack of practical interpretation, since the range of possible “beauty” scores does not include 0.</p>
</div>
<div id="sampling-scenario" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Sampling scenario</h3>
<p>Let’s now revisit this study in terms of the terminology and notation related to sampling we studied in Subsection <a href="#terminology-and-notation"><strong>??</strong></a>.</p>
<p>First, let’s view the instructors for these 463 courses as a <em>representative sample</em> from a greater <em>study population</em>. In our case, let’s assume that the study population is <em>all</em> instructors at UT Austin and that the sample of instructors who taught these 463 courses is a representative sample. Unfortunately, we can only <em>assume</em> these two facts without more knowledge of the <em>sampling methodology</em> used by the researchers.</p>
<p>Since we are viewing these <span class="math inline">\(n\)</span> = 463 courses as a sample, we can view our fitted slope <span class="math inline">\(b_1\)</span> = 0.067 as a <em>point estimate</em> of the <em>population slope</em> <span class="math inline">\(\beta_1\)</span>. In other words, <span class="math inline">\(\beta_1\)</span> quantifies the relationship between teaching <code>score</code> and “beauty” average <code>bty_avg</code> for <em>all</em> instructors at UT Austin. Similarly, we can view our fitted intercept <span class="math inline">\(b_0\)</span> = 3.88 as a <em>point estimate</em> of the <em>population intercept</em> <span class="math inline">\(\beta_0\)</span> for <em>all</em> instructors at UT Austin.</p>
<p>Putting these two ideas together, we can view the equation of the fitted line <span class="math inline">\(\widehat{y}\)</span> = <span class="math inline">\(b_0 + b_1 \cdot x\)</span> = <span class="math inline">\(3.880 + 0.067 \cdot \text{bty}\_\text{avg}\)</span> as an estimate of some true and unknown <em>population line</em> <span class="math inline">\(y = \beta_0 + \beta_1 \cdot x\)</span>. Thus we can draw parallels between our teaching evaluations analysis and all the sampling scenarios we’ve seen previously. In this chapter, we’ll focus on the final scenario of regression slopes as shown in Table <a href="1-inference-for-regression.html#tab:summarytable-ch11">1.2</a>.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:summarytable-ch11">TABLE 1.2: </span>Scenarios of sampling for inference
</caption>
<thead>
<tr>
<th style="text-align:right;">
Scenario
</th>
<th style="text-align:left;">
Population parameter
</th>
<th style="text-align:left;">
Notation
</th>
<th style="text-align:left;">
Point estimate
</th>
<th style="text-align:left;">
Symbol(s)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;width: 0.5in; ">
1
</td>
<td style="text-align:left;width: 0.7in; ">
Population proportion
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(p\)</span>
</td>
<td style="text-align:left;width: 1.1in; ">
Sample proportion
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\widehat{p}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
2
</td>
<td style="text-align:left;width: 0.7in; ">
Population mean
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\mu\)</span>
</td>
<td style="text-align:left;width: 1.1in; ">
Sample mean
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\overline{x}\)</span> or <span class="math inline">\(\widehat{\mu}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
3
</td>
<td style="text-align:left;width: 0.7in; ">
Difference in population proportions
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(p_1 - p_2\)</span>
</td>
<td style="text-align:left;width: 1.1in; ">
Difference in sample proportions
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\widehat{p}_1 - \widehat{p}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
4
</td>
<td style="text-align:left;width: 0.7in; ">
Difference in population means
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\mu_1 - \mu_2\)</span>
</td>
<td style="text-align:left;width: 1.1in; ">
Difference in sample means
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\overline{x}_1 - \overline{x}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
5
</td>
<td style="text-align:left;width: 0.7in; ">
Population regression slope
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(\beta_1\)</span>
</td>
<td style="text-align:left;width: 1.1in; ">
Fitted regression slope
</td>
<td style="text-align:left;width: 1in; ">
<span class="math inline">\(b_1\)</span> or <span class="math inline">\(\widehat{\beta}_1\)</span>
</td>
</tr>
</tbody>
</table>
<p>Since we are now viewing our fitted slope <span class="math inline">\(b_1\)</span> and fitted intercept <span class="math inline">\(b_0\)</span> as <em>point estimates</em> based on a <em>sample</em>, these estimates will again be subject to <em>sampling variability</em>. In other words, if we collected a new sample of data on a different set of <span class="math inline">\(n\)</span> = 463 courses and their instructors, the new fitted slope <span class="math inline">\(b_1\)</span> will likely differ from 0.067. The same goes for the new fitted intercept <span class="math inline">\(b_0\)</span>. But by how much will these estimates <em>vary</em>? This information is in the remaining columns of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>. Our knowledge of sampling from Chapter <a href="#sampling"><strong>??</strong></a>, confidence intervals from Chapter <a href="#confidence-intervals"><strong>??</strong></a>, and hypothesis tests from Chapter <a href="#hypothesis-testing"><strong>??</strong></a> will help us interpret these remaining columns.</p>
</div>
</div>
<div id="regression-interp" class="section level2">
<h2><span class="header-section-number">1.2</span> Interpreting regression tables</h2>
<p>We’ve so far focused only on the two leftmost columns of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>: <code>term</code> and <code>estimate</code>. Let’s now shift our attention to the remaining columns: <code>std_error</code>, <code>statistic</code>, <code>p_value</code>, <code>lower_ci</code> and <code>upper_ci</code> in Table <a href="1-inference-for-regression.html#tab:score-model-part-deux">1.3</a>.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:score-model-part-deux">TABLE 1.3: </span>Previously seen regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
3.880
</td>
<td style="text-align:right;">
0.076
</td>
<td style="text-align:right;">
50.96
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3.731
</td>
<td style="text-align:right;">
4.030
</td>
</tr>
<tr>
<td style="text-align:left;">
bty_avg
</td>
<td style="text-align:right;">
0.067
</td>
<td style="text-align:right;">
0.016
</td>
<td style="text-align:right;">
4.09
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.035
</td>
<td style="text-align:right;">
0.099
</td>
</tr>
</tbody>
</table>
<p>Given the lack of practical interpretation for the fitted intercept <span class="math inline">\(b_0\)</span>, in this section we’ll focus only on the second row of the table corresponding to the fitted slope <span class="math inline">\(b_1\)</span>. We’ll first interpret the <code>std_error</code>, <code>statistic</code>, <code>p_value</code>, <code>lower_ci</code> and <code>upper_ci</code> columns. Afterwards in the upcoming Subsection <a href="1-inference-for-regression.html#regression-table-computation">1.2.5</a>, we’ll discuss how R computes these values.</p>
<div id="regression-se" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Standard error</h3>
<p>The third column of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> <code>std_error</code> corresponds to the <em>standard error</em> of our estimates. Recall the definition of <strong>standard error</strong>  we saw in Subsection <a href="#sampling-definitions"><strong>??</strong></a>:</p>
<blockquote>
<p>The <em>standard error</em> is the standard deviation of any point estimate computed from a sample.</p>
</blockquote>
<p>So what does this mean in terms of the fitted slope <span class="math inline">\(b_1\)</span> = 0.067? This value is just one possible value of the fitted slope resulting from <em>this particular sample</em> of <span class="math inline">\(n\)</span> = 463 pairs of teaching and beauty scores. However, if we collected a different sample of <span class="math inline">\(n\)</span> = 463 pairs of teaching and beauty scores, we will almost certainly obtain a different fitted slope <span class="math inline">\(b_1\)</span>. This is due to <em>sampling variability</em>.</p>
<p>Say we hypothetically collected 1000 such samples of pairs of teaching and beauty scores, computed the 1000 resulting values of the fitted slope <span class="math inline">\(b_1\)</span>, and visualized them in a histogram. This would be a visualization of the <em>sampling distribution</em> of <span class="math inline">\(b_1\)</span>, which we defined in Subsection <a href="#sampling-definitions"><strong>??</strong></a>. Further recall that the standard deviation of the <em>sampling distribution</em> of <span class="math inline">\(b_1\)</span> has a special name: the <em>standard error</em>.</p>
<p>Recall that we constructed three sampling distributions for the sample proportion <span class="math inline">\(\widehat{p}\)</span> using shovels of size 25, 50, and 100 in Figure <a href="#fig:comparing-sampling-distributions"><strong>??</strong></a>. We observed that as the sample size increased, the standard error decreased as evidenced by the narrowing sampling distribution.</p>
<p>The <em>standard error</em> of <span class="math inline">\(b_1\)</span> similarly quantifies how much variation in the fitted slope <span class="math inline">\(b_1\)</span> one would expect between different samples. So in our case, we can expect about 0.016 units of variation in the <code>bty_avg</code> slope variable. Recall that the <code>estimate</code> and <code>std_error</code> values play a key role in <em>inferring</em> the value of the unknown population slope <span class="math inline">\(\beta_1\)</span> relating to <em>all</em> instructors.</p>
<p>In Section <a href="1-inference-for-regression.html#infer-regression">1.4</a>, we’ll perform a simulation using the <code>infer</code> package to construct the bootstrap distribution for <span class="math inline">\(b_1\)</span> in this case. Recall from Subsection <a href="#bootstrap-vs-sampling"><strong>??</strong></a> that the bootstrap distribution is an <em>approximation</em> to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar <em>standard errors</em>. However, unlike the sampling distribution, the bootstrap distribution is constructed from a <em>single</em> sample, which is a practice more aligned with what’s done in real life.</p>
</div>
<div id="regression-test-statistic" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Test statistic</h3>
<p>The fourth column of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> <code>statistic</code> corresponds to a <em>test statistic</em> relating to the following <em>hypothesis test</em>:</p>
<p><span class="math display">\[
\begin{aligned}
H_0 &amp;: \beta_1 = 0\\
\text{vs } H_A&amp;: \beta_1 \neq 0.
\end{aligned}
\]</span></p>
<p>Recall our terminology, notation, and definitions related to hypothesis tests we introduced in Section <a href="#understanding-ht"><strong>??</strong></a>.</p>
<blockquote>
<p>A <em>hypothesis test</em> consists of a test between two competing hypotheses: (1) a <em>null hypothesis</em> <span class="math inline">\(H_0\)</span> versus (2) an <em>alternative hypothesis</em> <span class="math inline">\(H_A\)</span>.</p>
<p>A <em>test statistic</em> is a point estimate/sample statistic formula used for hypothesis testing.</p>
</blockquote>
<p>Here, our <em>null hypothesis</em> <span class="math inline">\(H_0\)</span> assumes that the population slope <span class="math inline">\(\beta_1\)</span> is 0. If the population slope <span class="math inline">\(\beta_1\)</span> is truly 0, then this is saying that there is <em>no true relationship</em> between teaching and “beauty” scores for <em>all</em> the instructors in our population. In other words, <span class="math inline">\(x\)</span> = “beauty” score would have no associated effect on <span class="math inline">\(y\)</span> = teaching score.
The <em>alternative hypothesis</em> <span class="math inline">\(H_A\)</span>, on the other hand, assumes that the population slope <span class="math inline">\(\beta_1\)</span> is not 0, meaning it could be either positive or negative. This suggests either a positive or negative relationship between teaching and “beauty” scores. Recall we called such alternative hypotheses <em>two-sided</em>. By convention, all hypothesis testing for regression assumes two-sided alternatives.</p>
<p>Recall our “hypothesized universe” of no gender discrimination we <em>assumed</em> in our <code>promotions</code> activity in Section <a href="#ht-activity"><strong>??</strong></a>. Similarly here when conducting this hypothesis test, we’ll assume a “hypothesized universe” where there is no relationship between teaching and “beauty” scores. In other words, we’ll assume the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> is true.</p>
<p>The <code>statistic</code> column in the regression table is a tricky one, however. It corresponds to a standardized <em>t-test statistic</em>, much like the <em>two-sample <span class="math inline">\(t\)</span> statistic</em> we saw in Subsection <a href="#theory-hypo"><strong>??</strong></a> where we used a theory-based method for conducting hypothesis tests. In both these cases, the <em>null distribution</em> can be mathematically proven to be a <em><span class="math inline">\(t\)</span>-distribution</em>. Since such test statistics are tricky for individuals new to statistical inference to study, we’ll skip this and jump into interpreting the <span class="math inline">\(p\)</span>-value. If you’re curious, we have included a discussion of this standardized <em>t-test statistic</em> in Subsection <a href="1-inference-for-regression.html#theory-regression">1.5.1</a>.</p>
</div>
<div id="p-value" class="section level3">
<h3><span class="header-section-number">1.2.3</span> p-value</h3>
<p>The fifth column of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> <code>p_value</code> corresponds to the <em>p-value</em> of the hypothesis test <span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_A: \beta_1 \neq 0\)</span>.</p>
<p>Again recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section <a href="#understanding-ht"><strong>??</strong></a>, let’s focus on the definition of the <span class="math inline">\(p\)</span>-value:</p>
<blockquote>
<p>A <em>p-value</em> is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic <em>assuming the null hypothesis <span class="math inline">\(H_0\)</span> is true</em>.</p>
</blockquote>
<p>Recall that you can intuitively think of the <span class="math inline">\(p\)</span>-value as quantifying how “extreme” the observed fitted slope of <span class="math inline">\(b_1\)</span> = 0.067 is in a “hypothesized universe” where there is no relationship between teaching and “beauty” scores.</p>
<p>Following the hypothesis testing procedure we outlined in Section <a href="#ht-interpretation"><strong>??</strong></a>, since the <span class="math inline">\(p\)</span>-value in this case is 0, for any choice of significance level <span class="math inline">\(\alpha\)</span> we would reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_A\)</span>. Using non-statistical language, this is saying: we reject the hypothesis that there is no relationship between teaching and “beauty” scores in favor of the hypothesis that there is. That is to say, the evidence suggests there is a significant relationship, one that is positive.</p>
<p>More precisely, however, the <span class="math inline">\(p\)</span>-value corresponds to how extreme the observed test statistic of 4.09 is when compared to the appropriate <em>null distribution</em>. In Section <a href="1-inference-for-regression.html#infer-regression">1.4</a>, we’ll perform a simulation using the <code>infer</code> package to construct the null distribution in this case.</p>
<p>An extra caveat here is that the results of this hypothesis test are only valid if certain “conditions for inference for regression” are met, which we’ll introduce shortly in Section <a href="1-inference-for-regression.html#regression-conditions">1.3</a>.</p>
</div>
<div id="confidence-interval" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Confidence interval</h3>
<p>The two rightmost columns of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> (<code>lower_ci</code> and <code>upper_ci</code>) correspond to the endpoints of the 95% <em>confidence interval</em> for the population slope <span class="math inline">\(\beta_1\)</span>. Recall our analogy of “nets are to fish” what “confidence intervals are to population parameters” from Section <a href="#ci-build-up"><strong>??</strong></a>. The resulting 95% confidence interval for <span class="math inline">\(\beta_1\)</span> of (0.035, 0.099) can be thought of as a range of plausible values for the population slope <span class="math inline">\(\beta_1\)</span> of the linear relationship between teaching and “beauty” scores.</p>
<p>As we introduced in Subsection <a href="#shorthand"><strong>??</strong></a> on the precise and shorthand interpretation of confidence intervals, the statistically precise interpretation of this confidence interval is: “if we repeated this sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population slope <span class="math inline">\(\beta_1\)</span>.” However, we’ll summarize this using our shorthand interpretation that “we’re 95% ‘confident’ that the true population slope <span class="math inline">\(\beta_1\)</span> lies between 0.035 and 0.099.”</p>
<p>Notice in this case that the resulting 95% confidence interval for <span class="math inline">\(\beta_1\)</span> of <span class="math inline">\((0.035, \, 0.099)\)</span> does not contain a very particular value: <span class="math inline">\(\beta_1\)</span> equals 0. Recall we mentioned that if the population regression slope <span class="math inline">\(\beta_1\)</span> is 0, this is equivalent to saying there is <em>no</em> relationship between teaching and “beauty” scores. Since <span class="math inline">\(\beta_1\)</span> = 0 is not in our plausible range of values for <span class="math inline">\(\beta_1\)</span>, we are inclined to believe that there, in fact, <em>is</em> a relationship between teaching and “beauty” scores and a positive one at that. So in this case, the conclusion about the population slope <span class="math inline">\(\beta_1\)</span> from the 95% confidence interval matches the conclusion from the hypothesis test: evidence suggests that there is a meaningful relationship between teaching and “beauty” scores.</p>
<p>Recall from Subsection <a href="#ci-width"><strong>??</strong></a>, however, that the <em>confidence level</em> is one of many factors that determine confidence interval widths. So for example, say we used a higher confidence level of 99% instead of 95%. The resulting confidence interval for <span class="math inline">\(\beta_1\)</span> would be wider and thus might now include 0. The lesson to remember here is that any confidence-interval-based conclusion depends highly on the confidence level used.</p>
<p>What are the calculations that went into computing the two endpoints of the 95% confidence interval for <span class="math inline">\(\beta_1\)</span>?</p>
<p>Recall our sampling bowl example from Subsection <a href="#theory-ci"><strong>??</strong></a> discussing <code>lower_ci</code> and <code>upper_ci</code>. Since the sampling and bootstrap distributions of the sample proportion <span class="math inline">\(\widehat{p}\)</span> were roughly normal, we could use the rule of thumb for bell-shaped distributions from Appendix <a href="#appendix-normal-curve"><strong>??</strong></a> to create a 95% confidence interval for <span class="math inline">\(p\)</span> with the following equation:</p>
<p><span class="math display">\[\widehat{p} \pm \text{MoE}_{\widehat{p}} = \widehat{p} \pm 1.96 \cdot \text{SE}_{\widehat{p}} = \widehat{p} \pm 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]</span></p>
<p>We can generalize this to other point estimates that have roughly normally shaped sampling and/or bootstrap distributions:</p>
<p><span class="math display">\[\text{point estimate} \pm \text{MoE} = \text{point estimate} \pm 1.96 \cdot \text{SE}.\]</span></p>
<p>We’ll show in Section <a href="1-inference-for-regression.html#infer-regression">1.4</a> that the sampling/bootstrap distribution for the fitted slope <span class="math inline">\(b_1\)</span> is in fact bell-shaped as well. Thus we can construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> with the following equation:</p>
<p><span class="math display">\[b_1 \pm \text{MoE}_{b_1} = b_1 \pm 1.96 \cdot \text{SE}_{b_1}.\]</span></p>
<p>What is the value of the standard error <span class="math inline">\(\text{SE}_{b_1}\)</span>? It is in fact in the third column of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>: 0.016. Thus</p>
<p><span class="math display">\[
\begin{aligned}
b_1 \pm 1.96 \cdot \text{SE}_{b_1} &amp;= 0.067 \pm 1.96 \cdot 0.016 = 0.067 \pm 0.031\\
&amp;= (0.036, 0.098)
\end{aligned}
\]</span></p>
<p>This closely matches the <span class="math inline">\((0.035, 0.099)\)</span> confidence interval in the last two columns of Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>.</p>
<p>Much like hypothesis tests, however, the results of this confidence interval also are only valid if the “conditions for inference for regression” to be discussed in Section <a href="1-inference-for-regression.html#regression-conditions">1.3</a> are met.</p>
</div>
<div id="regression-table-computation" class="section level3">
<h3><span class="header-section-number">1.2.5</span> How does R compute the table?</h3>
<p>Since we didn’t perform the simulation to get the values of the standard error, test statistic, <span class="math inline">\(p\)</span>-value, and endpoints of the 95% confidence interval in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>, you might be wondering how were these values computed. What did R do behind the scenes? Does R run simulations like we did using the <code>infer</code> package in Chapters <a href="#confidence-intervals"><strong>??</strong></a> and <a href="#hypothesis-testing"><strong>??</strong></a> on confidence intervals and hypothesis testing?</p>
<p>The answer is no! Much like the theory-based method for constructing confidence intervals you saw in Subsection <a href="#theory-ci"><strong>??</strong></a> and the theory-based hypothesis test you saw in Subsection <a href="#theory-hypo"><strong>??</strong></a>, there exist mathematical formulas that allow you to construct confidence intervals and conduct hypothesis tests for inference for regression. These formulas were derived in a time when computers didn’t exist, so it would’ve been impossible to run the extensive computer simulations we have in this book. We present these formulas in Subsection <a href="1-inference-for-regression.html#theory-regression">1.5.1</a> on “theory-based inference for regression.”</p>
<p>In Section <a href="1-inference-for-regression.html#infer-regression">1.4</a>, we’ll go over a simulation-based approach to constructing confidence intervals and conducting hypothesis tests using the <code>infer</code> package. In particular, we’ll convince you that the bootstrap distribution of the fitted slope <span class="math inline">\(b_1\)</span> is indeed bell-shaped.</p>
</div>
</div>
<div id="regression-conditions" class="section level2">
<h2><span class="header-section-number">1.3</span> Conditions for inference for regression</h2>
<p>Recall in Subsection <a href="#se-method"><strong>??</strong></a> we stated that we could only use the standard-error-based method for constructing confidence intervals if the bootstrap distribution was bell shaped. Similarly, there are certain conditions that need to be met in order for the results of our hypothesis tests and confidence intervals we described in Section <a href="1-inference-for-regression.html#regression-interp">1.2</a> to have valid meaning. These conditions must be met for the assumed underlying mathematical and probability theory to hold true.</p>
<p>For inference for regression, there are four conditions that need to be met. Note the first four letters of these conditions are highlighted in bold in what follows: <strong>LINE</strong>. This can serve as a nice reminder of what to check for whenever you perform linear regression. </p>
<ol style="list-style-type: decimal">
<li><strong>L</strong>inearity of relationship between variables</li>
<li><strong>I</strong>ndependence of the residuals</li>
<li><strong>N</strong>ormality of the residuals</li>
<li><strong>E</strong>quality of variance of the residuals</li>
</ol>
<p>Conditions <strong>L</strong>, <strong>N</strong>, and <strong>E</strong> can be verified through what is known as a <em>residual analysis</em>. Condition <strong>I</strong> can only be verified through an understanding of how the data was collected.</p>
<p>In this section, we’ll go over a refresher on residuals, verify whether each of the four <strong>LINE</strong> conditions hold true, and then discuss the implications.</p>
<div id="residuals-refresher" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Residuals refresher</h3>
<p>Recall our definition of a residual from Subsection <a href="#model1points"><strong>??</strong></a>: it is the <em>observed value</em> minus the <em>fitted value</em> denoted by <span class="math inline">\(y - \widehat{y}\)</span>. Recall that residuals can be thought of as the error or the “lack-of-fit” between the observed value <span class="math inline">\(y\)</span> and the fitted value <span class="math inline">\(\widehat{y}\)</span> on the regression line in Figure <a href="1-inference-for-regression.html#fig:regline">1.1</a>. In Figure <a href="1-inference-for-regression.html#fig:residual-example">1.2</a>, we illustrate one particular residual out of 463 using an arrow, as well as its corresponding observed and fitted values using a circle and a square, respectively.</p>
<div class="figure" style="text-align: center"><span id="fig:residual-example"></span>
<img src="ModernDive_files/figure-html/residual-example-1.png" alt="Example of observed value, fitted value, and residual." width="\textwidth" />
<p class="caption">
FIGURE 1.2: Example of observed value, fitted value, and residual.
</p>
</div>
<p>Furthermore, we can automate the calculation of all <span class="math inline">\(n\)</span> = 463 residuals by applying the <code>get_regression_points()</code> function to our saved regression model in <code>score_model</code>. Observe how the resulting values of <code>residual</code> are roughly equal to <code>score - score_hat</code> (there is potentially a slight difference due to rounding error).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="1-inference-for-regression.html#cb6-1"></a><span class="co"># Fit regression model:</span></span>
<span id="cb6-2"><a href="1-inference-for-regression.html#cb6-2"></a>score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch5)</span>
<span id="cb6-3"><a href="1-inference-for-regression.html#cb6-3"></a><span class="co"># Get regression points:</span></span>
<span id="cb6-4"><a href="1-inference-for-regression.html#cb6-4"></a>regression_points &lt;-<span class="st"> </span><span class="kw">get_regression_points</span>(score_model)</span>
<span id="cb6-5"><a href="1-inference-for-regression.html#cb6-5"></a>regression_points</span></code></pre></div>
<pre><code># A tibble: 463 x 5
      ID score bty_avg score_hat residual
   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
 1     1   4.7    5         4.21    0.486
 2     2   4.1    5         4.21   -0.114
 3     3   3.9    5         4.21   -0.314
 4     4   4.8    5         4.21    0.586
 5     5   4.6    3         4.08    0.52 
 6     6   4.3    3         4.08    0.22 
 7     7   2.8    3         4.08   -1.28 
 8     8   4.1    3.33      4.10   -0.002
 9     9   3.4    3.33      4.10   -0.702
10    10   4.5    3.17      4.09    0.409
# … with 453 more rows</code></pre>
<p>A <em>residual analysis</em> is used to verify conditions <strong>L</strong>, <strong>N</strong>, and <strong>E</strong> and can be performed using appropriate data visualizations. While there are more sophisticated statistical approaches that can also be done, we’ll focus on the much simpler approach of looking at plots.</p>
</div>
<div id="linearity-of-relationship" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Linearity of relationship</h3>
<p>The first condition is that the relationship between the outcome variable <span class="math inline">\(y\)</span> and the explanatory variable <span class="math inline">\(x\)</span> must be <strong>L</strong>inear. Recall the scatterplot in Figure <a href="1-inference-for-regression.html#fig:regline">1.1</a> where we had the explanatory variable <span class="math inline">\(x\)</span> as “beauty” score and the outcome variable <span class="math inline">\(y\)</span> as teaching score. Would you say that the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is linear? It’s hard to say because of the scatter of the points about the line. In the authors’ opinions, we feel this relationship is “linear enough.”</p>
<p>Let’s present an example where the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is clearly not linear in Figure <a href="1-inference-for-regression.html#fig:non-linear">1.3</a>. In this case, the points clearly do not form a line, but rather a U-shaped polynomial curve. In this case, any results from an inference for regression would not be valid.</p>
<div class="figure" style="text-align: center"><span id="fig:non-linear"></span>
<img src="ModernDive_files/figure-html/non-linear-1.png" alt="Example of a clearly non-linear relationship." width="\textwidth" />
<p class="caption">
FIGURE 1.3: Example of a clearly non-linear relationship.
</p>
</div>
</div>
<div id="independence-of-residuals" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Independence of residuals</h3>
<p>The second condition is that the residuals must be <strong>I</strong>ndependent. In other words, the different observations in our data must be independent of one another.</p>
<p>For our UT Austin data, while there is data on 463 courses, these 463 courses were actually taught by 94 unique instructors. In other words, the same professor is often included more than once in our data. The original <code>evals</code> data frame that we used to construct the <code>evals_ch5</code> data frame has a variable <code>prof_ID</code>, which is an anonymized identification variable for the professor:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="1-inference-for-regression.html#cb8-1"></a>evals <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-2"><a href="1-inference-for-regression.html#cb8-2"></a><span class="st">  </span><span class="kw">select</span>(ID, prof_ID, score, bty_avg)</span></code></pre></div>
<pre><code># A tibble: 463 x 4
      ID prof_ID score bty_avg
   &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;
 1     1       1   4.7    5   
 2     2       1   4.1    5   
 3     3       1   3.9    5   
 4     4       1   4.8    5   
 5     5       2   4.6    3   
 6     6       2   4.3    3   
 7     7       2   2.8    3   
 8     8       3   4.1    3.33
 9     9       3   3.4    3.33
10    10       4   4.5    3.17
# … with 453 more rows</code></pre>
<p>For example, the professor with <code>prof_ID</code> equal to 1 taught the first 4 courses in the data, the professor with <code>prof_ID</code> equal to 2 taught the next 3, and so on. Given that the same professor taught these first four courses, it is reasonable to expect that these four teaching scores are related to each other. If a professor gets a high <code>score</code> in one class, chances are fairly good they’ll get a high <code>score</code> in another. This dataset thus provides different information than if we had 463 unique instructors teaching the 463 courses.</p>
<p>In this case, we say there exists <em>dependence</em> between observations. The first four courses taught by professor 1 are dependent, the next 3 courses taught by professor 2 are related, and so on. Any proper analysis of this data needs to take into account that we have <em>repeated measures</em> for the same profs.</p>
<p>So in this case, the independence condition is not met. What does this mean for our analysis? We’ll address this in Subsection <a href="1-inference-for-regression.html#what-is-the-conclusion">1.3.6</a> coming up, after we check the remaining two conditions.</p>
</div>
<div id="normality-of-residuals" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Normality of residuals</h3>
<p>The third condition is that the residuals should follow a <strong>N</strong>ormal distribution. Furthermore, the center of this distribution should be 0. In other words, sometimes the regression model will make positive errors: <span class="math inline">\(y - \widehat{y} &gt; 0\)</span>. Other times, the regression model will make equally negative errors: <span class="math inline">\(y - \widehat{y} &lt; 0\)</span>. However, <em>on average</em> the errors should equal 0 and their shape should be similar to that of a bell.</p>
<p>The simplest way to check the normality of the residuals is to look at a histogram, which we visualize in Figure <a href="1-inference-for-regression.html#fig:model1residualshist">1.4</a>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="1-inference-for-regression.html#cb10-1"></a><span class="kw">ggplot</span>(regression_points, <span class="kw">aes</span>(<span class="dt">x =</span> residual)) <span class="op">+</span></span>
<span id="cb10-2"><a href="1-inference-for-regression.html#cb10-2"></a><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="fl">0.25</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb10-3"><a href="1-inference-for-regression.html#cb10-3"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Residual&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:model1residualshist"></span>
<img src="ModernDive_files/figure-html/model1residualshist-1.png" alt="Histogram of residuals." width="\textwidth" />
<p class="caption">
FIGURE 1.4: Histogram of residuals.
</p>
</div>
<p>This histogram shows that we have more positive residuals than negative. Since the residual <span class="math inline">\(y-\widehat{y}\)</span> is positive when <span class="math inline">\(y &gt; \widehat{y}\)</span>, it seems our regression model’s fitted teaching scores <span class="math inline">\(\widehat{y}\)</span> tend to <em>underestimate</em> the true teaching scores <span class="math inline">\(y\)</span>. Furthermore, this histogram has a slight <em>left-skew</em> in that there is a tail on the left. This is another way to say the residuals exhibit a <em>negative skew</em>.</p>
<p>Is this a problem? Again, there is a certain amount of subjectivity in the response. In the authors’ opinion, while there is a slight skew to the residuals, we feel it isn’t drastic. On the other hand, others might disagree with our assessment.</p>
<p>Let’s present examples where the residuals clearly do and don’t follow a normal distribution in Figure <a href="1-inference-for-regression.html#fig:normal-residuals">1.5</a>. In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid.</p>
<div class="figure" style="text-align: center"><span id="fig:normal-residuals"></span>
<img src="ModernDive_files/figure-html/normal-residuals-1.png" alt="Example of clearly normal and clearly not normal residuals." width="\textwidth" />
<p class="caption">
FIGURE 1.5: Example of clearly normal and clearly not normal residuals.
</p>
</div>
</div>
<div id="equality-of-variance" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Equality of variance</h3>
<p>The fourth and final condition is that the residuals should exhibit <strong>E</strong>qual variance across all values of the explanatory variable <span class="math inline">\(x\)</span>. In other words, the value and spread of the residuals should not depend on the value of the explanatory variable <span class="math inline">\(x\)</span>.</p>
<p>Recall the scatterplot in Figure <a href="1-inference-for-regression.html#fig:regline">1.1</a>: we had the explanatory variable <span class="math inline">\(x\)</span> of “beauty” score on the x-axis and the outcome variable <span class="math inline">\(y\)</span> of teaching score on the y-axis. Instead, let’s create a scatterplot that has the same values on the x-axis, but now with the residual <span class="math inline">\(y-\widehat{y}\)</span> on the y-axis as seen in Figure <a href="1-inference-for-regression.html#fig:numxplot6">1.6</a>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="1-inference-for-regression.html#cb11-1"></a><span class="kw">ggplot</span>(regression_points, <span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> residual)) <span class="op">+</span></span>
<span id="cb11-2"><a href="1-inference-for-regression.html#cb11-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb11-3"><a href="1-inference-for-regression.html#cb11-3"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Beauty Score&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Residual&quot;</span>) <span class="op">+</span></span>
<span id="cb11-4"><a href="1-inference-for-regression.html#cb11-4"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:numxplot6"></span>
<img src="ModernDive_files/figure-html/numxplot6-1.png" alt="Plot of residuals over beauty score." width="\textwidth" />
<p class="caption">
FIGURE 1.6: Plot of residuals over beauty score.
</p>
</div>
<p>You can think of Figure <a href="1-inference-for-regression.html#fig:numxplot6">1.6</a> as a modified version of the plot with the regression line in Figure <a href="1-inference-for-regression.html#fig:regline">1.1</a>, but with the regression line flattened out to <span class="math inline">\(y=0\)</span>. Looking at this plot, would you say that the spread of the residuals around the line at <span class="math inline">\(y=0\)</span> is constant across all values of the explanatory variable <span class="math inline">\(x\)</span> of “beauty” score? This question is rather qualitative and subjective in nature, thus different people may respond with different answers. For example, some people might say that there is slightly more variation in the residuals for smaller values of <span class="math inline">\(x\)</span> than for higher ones. However, it can be argued that there isn’t a <em>drastic</em> non-constancy.</p>
<p>In Figure <a href="1-inference-for-regression.html#fig:equal-variance-residuals">1.7</a> let’s present an example where the residuals clearly do not have equal variance across all values of the explanatory variable <span class="math inline">\(x\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:equal-variance-residuals"></span>
<img src="ModernDive_files/figure-html/equal-variance-residuals-1.png" alt="Example of clearly non-equal variance." width="\textwidth" />
<p class="caption">
FIGURE 1.7: Example of clearly non-equal variance.
</p>
</div>
<p>Observe how the spread of the residuals increases as the value of <span class="math inline">\(x\)</span> increases. This is a situation known as  <em>heteroskedasticity</em>. Any inference for regression based on a model yielding such a pattern in the residuals would not be valid.</p>
</div>
<div id="what-is-the-conclusion" class="section level3">
<h3><span class="header-section-number">1.3.6</span> What’s the conclusion?</h3>
<p>Let’s list our four conditions for inference for regression again and indicate whether or not they were satisfied in our analysis:</p>
<ol style="list-style-type: decimal">
<li><strong>L</strong>inearity of relationship between variables: Yes</li>
<li><strong>I</strong>ndependence of residuals: No</li>
<li><strong>N</strong>ormality of residuals: Somewhat</li>
<li><strong>E</strong>quality of variance: Yes</li>
</ol>
<p>So what does this mean for the results of our confidence intervals and hypothesis tests in Section <a href="1-inference-for-regression.html#regression-interp">1.2</a>?</p>
<p>First, the <strong>I</strong>ndependence condition. The fact that there exist dependencies between different rows in <code>evals_ch5</code> must be addressed. In more advanced statistics courses, you’ll learn how to incorporate such dependencies into your regression models. One such technique is called <em>hierarchical/multilevel modeling</em>.</p>
<p>Second, when conditions <strong>L</strong>, <strong>N</strong>, <strong>E</strong> are not met, it often means there is a shortcoming in our model. For example, it may be the case that using only a single explanatory variable is insufficient, as we did with “beauty” score. We may need to incorporate more explanatory variables in a multiple regression model as we did in Chapter <a href="#multiple-regression"><strong>??</strong></a>.</p>
<p>In our case, the best we can do is view the results suggested by our confidence intervals and hypothesis tests as preliminary. While a preliminary analysis suggests that there is a significant relationship between teaching and “beauty” scores, further investigation is warranted; in particular, by improving the preliminary <code>score ~ bty_avg</code> model so that the four conditions are met. When the four conditions are roughly met, then we can put more faith into our confidence intervals and <span class="math inline">\(p\)</span>-values.</p>
<p>The conditions for inference in regression problems are a key part of regression analysis that are of vital importance to the processes of constructing confidence intervals and conducting hypothesis tests. However, it is often the case with regression analysis in the real world that not all the conditions are completely met. Furthermore, as you saw, there is a level of subjectivity in the residual analyses to verify the <strong>L</strong>, <strong>N</strong>, and <strong>E</strong> conditions. So what can you do? We as authors advocate for transparency in communicating all results. This lets the stakeholders of any analysis know about a model’s shortcomings or whether the model is “good enough.” So while this checking of assumptions has lead to some fuzzy “it depends” results, we decided as authors to show you these scenarios to help prepare you for difficult statistical decisions you may need to make down the road.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.1)</strong> Continuing with our regression using <code>age</code> as the explanatory variable and teaching <code>score</code> as the outcome variable.</p>
<ul>
<li>Use the <code>get_regression_points()</code> function to get the observed values, fitted values, and residuals for all 463 instructors.</li>
<li>Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.</li>
</ul>
<div class="learncheck">

</div>
</div>
</div>
<div id="infer-regression" class="section level2">
<h2><span class="header-section-number">1.4</span> Simulation-based inference for regression</h2>
<p>Recall in Subsection <a href="1-inference-for-regression.html#regression-table-computation">1.2.5</a> when we interpreted the third through seventh columns of a regression table, we stated that R doesn’t do simulations to compute these values. Rather R uses theory-based methods that involve mathematical formulas.</p>
<p>In this section, we’ll use the simulation-based methods you previously learned in Chapters <a href="#confidence-intervals"><strong>??</strong></a> and <a href="#hypothesis-testing"><strong>??</strong></a> to recreate the values in the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>. In particular, we’ll use the <code>infer</code> package workflow to</p>
<ul>
<li>Construct a 95% confidence interval for the population slope <span class="math inline">\(\beta_1\)</span> using bootstrap resampling with replacement. We did this previously in Sections <a href="#bootstrap-process"><strong>??</strong></a> with the <code>pennies</code> data and <a href="#case-study-two-prop-ci"><strong>??</strong></a> with the <code>mythbusters_yawn</code> data.</li>
<li>Conduct a hypothesis test of <span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_A: \beta_1 \neq 0\)</span> using a permutation test. We did this previously in Sections <a href="#ht-infer"><strong>??</strong></a> with the <code>promotions</code> data and <a href="#ht-case-study"><strong>??</strong></a> with the <code>movies_sample</code> IMDb data.</li>
</ul>
<div id="confidence-interval-for-slope" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Confidence interval for slope</h3>
<p>We’ll construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> using the <code>infer</code> workflow outlined in Subsection <a href="#infer-workflow"><strong>??</strong></a>. Specifically, we’ll first construct the bootstrap distribution for the fitted slope <span class="math inline">\(b_1\)</span> using our single sample of 463 courses:</p>
<ol style="list-style-type: decimal">
<li><code>specify()</code> the variables of interest in <code>evals_ch5</code> with the formula: <code>score ~ bty_avg</code>.</li>
<li><code>generate()</code> replicates by using <code>bootstrap</code> resampling with replacement from the original sample of 463 courses. We generate <code>reps = 1000</code> replicates using <code>type = "bootstrap"</code>.</li>
<li><code>calculate()</code> the summary statistic of interest: the fitted <code>slope</code> <span class="math inline">\(b_1\)</span>.</li>
</ol>
<p>Using this bootstrap distribution, we’ll construct the 95% confidence interval using the percentile method and (if appropriate) the standard error method as well. It is important to note in this case that the bootstrapping with replacement is done <em>row-by-row</em>. Thus, the original pairs of <code>score</code> and <code>bty_avg</code> values are always kept together, but different pairs of <code>score</code> and <code>bty_avg</code> values may be resampled multiple times. The resulting confidence interval will denote a range of plausible values for the unknown population slope <span class="math inline">\(\beta_1\)</span> quantifying the relationship between teaching and “beauty” scores for <em>all</em> professors at UT Austin.</p>
<p>Let’s first construct the bootstrap distribution for the fitted slope <span class="math inline">\(b_1\)</span>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="1-inference-for-regression.html#cb12-1"></a>bootstrap_distn_slope &lt;-<span class="st"> </span>evals_ch5 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-2"><a href="1-inference-for-regression.html#cb12-2"></a><span class="st">  </span><span class="kw">specify</span>(<span class="dt">formula =</span> score <span class="op">~</span><span class="st"> </span>bty_avg) <span class="op">%&gt;%</span></span>
<span id="cb12-3"><a href="1-inference-for-regression.html#cb12-3"></a><span class="st">  </span><span class="kw">generate</span>(<span class="dt">reps =</span> <span class="dv">1000</span>, <span class="dt">type =</span> <span class="st">&quot;bootstrap&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb12-4"><a href="1-inference-for-regression.html#cb12-4"></a><span class="st">  </span><span class="kw">calculate</span>(<span class="dt">stat =</span> <span class="st">&quot;slope&quot;</span>)</span>
<span id="cb12-5"><a href="1-inference-for-regression.html#cb12-5"></a>bootstrap_distn_slope</span></code></pre></div>
<pre><code># A tibble: 1,000 x 2
   replicate   stat
       &lt;int&gt;  &lt;dbl&gt;
 1         1 0.0651
 2         2 0.0382
 3         3 0.108 
 4         4 0.0667
 5         5 0.0716
 6         6 0.0855
 7         7 0.0625
 8         8 0.0413
 9         9 0.0796
10        10 0.0761
# … with 990 more rows</code></pre>
<p>Observe how we have 1000 values of the bootstrapped slope <span class="math inline">\(b_1\)</span> in the <code>stat</code> column. Let’s visualize the 1000 bootstrapped values in Figure <a href="1-inference-for-regression.html#fig:bootstrap-distribution-slope">1.8</a>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="1-inference-for-regression.html#cb14-1"></a><span class="kw">visualize</span>(bootstrap_distn_slope)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:bootstrap-distribution-slope"></span>
<img src="ModernDive_files/figure-html/bootstrap-distribution-slope-1.png" alt="Bootstrap distribution of slope." width="\textwidth" />
<p class="caption">
FIGURE 1.8: Bootstrap distribution of slope.
</p>
</div>
<p>Observe how the bootstrap distribution is roughly bell-shaped. Recall from Subsection <a href="#bootstrap-vs-sampling"><strong>??</strong></a> that the shape of the bootstrap distribution of <span class="math inline">\(b_1\)</span> closely approximates the shape of the sampling distribution of <span class="math inline">\(b_1\)</span>.</p>
<div id="percentile-method" class="section level4 unnumbered">
<h4>Percentile-method</h4>
<p>First, let’s compute the 95% confidence interval for <span class="math inline">\(\beta_1\)</span> using the percentile method. We’ll do so by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="1-inference-for-regression.html#cb15-1"></a>percentile_ci &lt;-<span class="st"> </span>bootstrap_distn_slope <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb15-2"><a href="1-inference-for-regression.html#cb15-2"></a><span class="st">  </span><span class="kw">get_confidence_interval</span>(<span class="dt">type =</span> <span class="st">&quot;percentile&quot;</span>, <span class="dt">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb15-3"><a href="1-inference-for-regression.html#cb15-3"></a>percentile_ci</span></code></pre></div>
<pre><code># A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1   0.0323   0.0990</code></pre>
<p>The resulting percentile-based 95% confidence interval for <span class="math inline">\(\beta_1\)</span> of (0.032, 0.099) is similar to the confidence interval in the regression Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> of (0.035, 0.099).</p>
</div>
<div id="standard-error-method" class="section level4 unnumbered">
<h4>Standard error method</h4>
<p>Since the bootstrap distribution in Figure <a href="1-inference-for-regression.html#fig:bootstrap-distribution-slope">1.8</a> appears to be roughly bell-shaped, we can also construct a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> using the standard error method.</p>
<p>In order to do this, we need to first compute the fitted slope <span class="math inline">\(b_1\)</span>, which will act as the center of our standard error-based confidence interval. While we saw in the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> that this was <span class="math inline">\(b_1\)</span> = 0.067, we can also use the <code>infer</code> pipeline with the <code>generate()</code> step removed to calculate it:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="1-inference-for-regression.html#cb17-1"></a>observed_slope &lt;-<span class="st"> </span>evals <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb17-2"><a href="1-inference-for-regression.html#cb17-2"></a><span class="st">  </span><span class="kw">specify</span>(score <span class="op">~</span><span class="st"> </span>bty_avg) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb17-3"><a href="1-inference-for-regression.html#cb17-3"></a><span class="st">  </span><span class="kw">calculate</span>(<span class="dt">stat =</span> <span class="st">&quot;slope&quot;</span>)</span>
<span id="cb17-4"><a href="1-inference-for-regression.html#cb17-4"></a>observed_slope</span></code></pre></div>
<pre><code># A tibble: 1 x 1
    stat
   &lt;dbl&gt;
1 0.0666</code></pre>
<p>We then use the <code>get_ci()</code> function with <code>level = 0.95</code> to compute the 95% confidence interval for <span class="math inline">\(\beta_1\)</span>. Note that setting the <code>point_estimate</code> argument to the <code>observed_slope</code> of 0.067 sets the center of the confidence interval.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="1-inference-for-regression.html#cb19-1"></a>se_ci &lt;-<span class="st"> </span>bootstrap_distn_slope <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb19-2"><a href="1-inference-for-regression.html#cb19-2"></a><span class="st">  </span><span class="kw">get_ci</span>(<span class="dt">level =</span> <span class="fl">0.95</span>, <span class="dt">type =</span> <span class="st">&quot;se&quot;</span>, <span class="dt">point_estimate =</span> observed_slope)</span>
<span id="cb19-3"><a href="1-inference-for-regression.html#cb19-3"></a>se_ci</span></code></pre></div>
<pre><code># A tibble: 1 x 2
  lower_ci upper_ci
     &lt;dbl&gt;    &lt;dbl&gt;
1   0.0334   0.0999</code></pre>
<p>The resulting standard error-based 95% confidence interval for <span class="math inline">\(\beta_1\)</span> of <span class="math inline">\((, )\)</span> is slightly different than the confidence interval in the regression Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> of <span class="math inline">\((0.035, 0.099)\)</span>.</p>
</div>
<div id="comparing-all-three" class="section level4 unnumbered">
<h4>Comparing all three</h4>
<p>Let’s compare all three confidence intervals in Figure <a href="1-inference-for-regression.html#fig:bootstrap-distribution-slope-CI">1.9</a>, where the percentile-based confidence interval is marked with solid lines, the standard error based confidence interval is marked with dashed lines, and the theory-based confidence interval (0.035, 0.099) from the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a> is marked with dotted lines.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="1-inference-for-regression.html#cb21-1"></a><span class="kw">visualize</span>(bootstrap_distn_slope) <span class="op">+</span><span class="st"> </span></span>
<span id="cb21-2"><a href="1-inference-for-regression.html#cb21-2"></a><span class="st">  </span><span class="kw">shade_confidence_interval</span>(<span class="dt">endpoints =</span> percentile_ci, <span class="dt">fill =</span> <span class="ot">NULL</span>, </span>
<span id="cb21-3"><a href="1-inference-for-regression.html#cb21-3"></a>                            <span class="dt">linetype =</span> <span class="st">&quot;solid&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;grey90&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb21-4"><a href="1-inference-for-regression.html#cb21-4"></a><span class="st">  </span><span class="kw">shade_confidence_interval</span>(<span class="dt">endpoints =</span> se_ci, <span class="dt">fill =</span> <span class="ot">NULL</span>, </span>
<span id="cb21-5"><a href="1-inference-for-regression.html#cb21-5"></a>                            <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;grey60&quot;</span>) <span class="op">+</span></span>
<span id="cb21-6"><a href="1-inference-for-regression.html#cb21-6"></a><span class="st">  </span><span class="kw">shade_confidence_interval</span>(<span class="dt">endpoints =</span> <span class="kw">c</span>(<span class="fl">0.035</span>, <span class="fl">0.099</span>), <span class="dt">fill =</span> <span class="ot">NULL</span>, </span>
<span id="cb21-7"><a href="1-inference-for-regression.html#cb21-7"></a>                            <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:bootstrap-distribution-slope-CI"></span>
<img src="ModernDive_files/figure-html/bootstrap-distribution-slope-CI-1.png" alt="Comparing three confidence intervals for the slope." width="\textwidth" />
<p class="caption">
FIGURE 1.9: Comparing three confidence intervals for the slope.
</p>
</div>
<p>Observe that all three are quite similar! Furthermore, none of the three confidence intervals for <span class="math inline">\(\beta_1\)</span> contain 0 and are entirely located above 0. This is suggesting that there is in fact a meaningful positive relationship between teaching and “beauty” scores.</p>
</div>
</div>
<div id="hypothesis-test-for-slope" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Hypothesis test for slope</h3>
<p>Let’s now conduct a hypothesis test of <span class="math inline">\(H_0: \beta_1 = 0\)</span> vs. <span class="math inline">\(H_A: \beta_1 \neq 0\)</span>. We will use the <code>infer</code> package, which follows the hypothesis testing paradigm in the “There is only one test” diagram in Figure <a href="#fig:htdowney"><strong>??</strong></a>.</p>
<p>Let’s first think about what it means for <span class="math inline">\(\beta_1\)</span> to be zero as assumed in the null hypothesis <span class="math inline">\(H_0\)</span>. Recall we said if <span class="math inline">\(\beta_1 = 0\)</span>, then this is saying there is no relationship between the teaching and “beauty” scores. Thus assuming this particular null hypothesis <span class="math inline">\(H_0\)</span> means that in our “hypothesized universe” there is no relationship between <code>score</code> and <code>bty_avg</code>. We can therefore shuffle/permute the <code>bty_avg</code> variable to no consequence.</p>
<p>We construct the null distribution of the fitted slope <span class="math inline">\(b_1\)</span> by performing the steps that follow. Recall from Section <a href="#understanding-ht"><strong>??</strong></a> on terminology, notation, and definitions related to hypothesis testing where we defined the <em>null distribution</em>: the sampling distribution of our test statistic <span class="math inline">\(b_1\)</span> assuming the null hypothesis <span class="math inline">\(H_0\)</span> is true.</p>
<ol style="list-style-type: decimal">
<li><code>specify()</code> the variables of interest in <code>evals_ch5</code> with the formula: <code>score ~ bty_avg</code>.</li>
<li><code>hypothesize()</code> the null hypothesis of <code>independence</code>. Recall from Section <a href="#ht-infer"><strong>??</strong></a> that this is an additional step that needs to be added for hypothesis testing.</li>
<li><code>generate()</code> replicates by permuting/shuffling values from the original sample of 463 courses. We generate <code>reps = 1000</code> replicates using <code>type = "permute"</code> here.</li>
<li><code>calculate()</code> the test statistic of interest: the fitted <code>slope</code> <span class="math inline">\(b_1\)</span>.</li>
</ol>
<p>In this case, we <code>permute</code> the values of <code>bty_avg</code> across the values of <code>score</code> 1000 times. We can do this shuffling/permuting since we assumed a “hypothesized universe” of no relationship between these two variables. Then we <code>calculate</code> the <code>"slope"</code> coefficient for each of these 1000 <code>generate</code>d samples.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="1-inference-for-regression.html#cb22-1"></a>null_distn_slope &lt;-<span class="st"> </span>evals <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb22-2"><a href="1-inference-for-regression.html#cb22-2"></a><span class="st">  </span><span class="kw">specify</span>(score <span class="op">~</span><span class="st"> </span>bty_avg) <span class="op">%&gt;%</span></span>
<span id="cb22-3"><a href="1-inference-for-regression.html#cb22-3"></a><span class="st">  </span><span class="kw">hypothesize</span>(<span class="dt">null =</span> <span class="st">&quot;independence&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb22-4"><a href="1-inference-for-regression.html#cb22-4"></a><span class="st">  </span><span class="kw">generate</span>(<span class="dt">reps =</span> <span class="dv">1000</span>, <span class="dt">type =</span> <span class="st">&quot;permute&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb22-5"><a href="1-inference-for-regression.html#cb22-5"></a><span class="st">  </span><span class="kw">calculate</span>(<span class="dt">stat =</span> <span class="st">&quot;slope&quot;</span>)</span></code></pre></div>
<p>Observe the resulting null distribution for the fitted slope <span class="math inline">\(b_1\)</span> in Figure <a href="1-inference-for-regression.html#fig:null-distribution-slope">1.10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:null-distribution-slope"></span>
<img src="ModernDive_files/figure-html/null-distribution-slope-1.png" alt="Null distribution of slopes." width="\textwidth" />
<p class="caption">
FIGURE 1.10: Null distribution of slopes.
</p>
</div>
<p>Notice how it is centered at <span class="math inline">\(b_1\)</span> = 0. This is because in our hypothesized universe, there is no relationship between <code>score</code> and <code>bty_avg</code> and so <span class="math inline">\(\beta_1 = 0\)</span>. Thus, the most typical fitted slope <span class="math inline">\(b_1\)</span> we observe across our simulations is 0. Observe, furthermore, how there is variation around this central value of 0.</p>
<p>Let’s visualize the <span class="math inline">\(p\)</span>-value in the null distribution by comparing it to the observed test statistic of <span class="math inline">\(b_1\)</span> = 0.067 in Figure <a href="1-inference-for-regression.html#fig:p-value-slope">1.11</a>. We’ll do this by adding a <code>shade_p_value()</code> layer to the previous <code>visualize()</code> code.</p>
<div class="figure" style="text-align: center"><span id="fig:p-value-slope"></span>
<img src="ModernDive_files/figure-html/p-value-slope-1.png" alt="Null distribution and $p$-value." width="\textwidth" />
<p class="caption">
FIGURE 1.11: Null distribution and <span class="math inline">\(p\)</span>-value.
</p>
</div>
<p>Since the observed fitted slope 0.067 falls far to the right of this null distribution and thus the shaded region doesn’t overlap it, we’ll have a <span class="math inline">\(p\)</span>-value of 0. For completeness, however, let’s compute the numerical value of the <span class="math inline">\(p\)</span>-value anyways using the <code>get_p_value()</code> function. Recall that it takes the same inputs as the <code>shade_p_value()</code> function:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="1-inference-for-regression.html#cb23-1"></a>null_distn_slope <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb23-2"><a href="1-inference-for-regression.html#cb23-2"></a><span class="st">  </span><span class="kw">get_p_value</span>(<span class="dt">obs_stat =</span> observed_slope, <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>)</span></code></pre></div>
<pre><code># A tibble: 1 x 1
  p_value
    &lt;dbl&gt;
1       0</code></pre>
<p>This matches the <span class="math inline">\(p\)</span>-value of 0 in the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>. We therefore reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> in favor of the alternative hypothesis <span class="math inline">\(H_A: \beta_1 \neq 0\)</span>. We thus have evidence that suggests there is a significant relationship between teaching and “beauty” scores for <em>all</em> instructors at UT Austin.</p>
<p>When the conditions for inference for regression are met and the null distribution has a bell shape, we are likely to see similar results between the simulation-based results we just demonstrated and the theory-based results shown in the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC10.2)</strong> Repeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of <code>stat = "correlation"</code> in the <code>calculate()</code> function of the <code>infer</code> package.</p>
<div class="learncheck">

</div>
</div>
</div>
<div id="inference-conclusion" class="section level2">
<h2><span class="header-section-number">1.5</span> Conclusion</h2>
<!--
v2 TODO: Consider adding

### Relating regression to other methods

To conclude this chapter, we'll be investigating how regression relates to two different statistical techniques. One of them was covered already in this book, the difference in sample means, and the other is new to the text but is related, ANOVA. We'll see how both can be represented in the regression framework. The hope is that this closing section helps you to tie together many of the concepts you've seen in the Data Modeling and Statistical Inference parts of this book.

#### Two sample difference in means

#### ANOVA


-->
<div id="theory-regression" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Theory-based inference for regression</h3>
<p>Recall in Subsection <a href="1-inference-for-regression.html#regression-table-computation">1.2.5</a> when we interpreted the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>, we mentioned that R does not compute its values using simulation-based methods for constructing confidence intervals and conducting hypothesis tests as we did in Chapters <a href="#confidence-intervals"><strong>??</strong></a> and <a href="#hypothesis-testing"><strong>??</strong></a> using the <code>infer</code> package. Rather, R uses a theory-based approach using mathematical formulas, much like the theory-based confidence intervals you saw in Subsection <a href="#theory-ci"><strong>??</strong></a> and the theory-based hypothesis tests you saw in Subsection <a href="#theory-hypo"><strong>??</strong></a>. These formulas were derived in a time when computers didn’t exist, so it would’ve been incredibly labor intensive to run extensive simulations.</p>
<p>In particular, there is a formula for the <em>standard error</em> of the fitted slope <span class="math inline">\(b_1\)</span>:</p>
<p><span class="math display">\[\text{SE}_{b_1} = \dfrac{\dfrac{s_y}{s_x} \cdot \sqrt{1-r^2}}{\sqrt{n-2}}\]</span></p>
<!-- Source: https://stats.stackexchange.com/questions/342632/how-to-understand-se-of-regression-slope-equation -->
<p>As with many formulas in statistics, there’s a lot going on here, so let’s first break down what each symbol represents. First <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the <em>sample standard deviations</em> of the explanatory variable <code>bty_avg</code> and the response variable <code>score</code>, respectively. Second, <span class="math inline">\(r\)</span> is the sample <em>correlation coefficient</em> between <code>score</code> and <code>bty_avg</code>. This was computed as 0.187 in Chapter <a href="#regression"><strong>??</strong></a>. Lastly, <span class="math inline">\(n\)</span> is the number of pairs of points in the <code>evals_ch5</code> data frame, here 463.</p>
<p>To put this formula into words, the standard error of <span class="math inline">\(b_1\)</span> depends on the relationship between the variability of the response variable and the variability of the explanatory variable as measured in the <span class="math inline">\(s_y / s_x\)</span> term. Next, it looks into how the two variables relate to each other in the <span class="math inline">\(\sqrt{1-r^2}\)</span> term.</p>
<p>However, the most important observation to make in the previous formula is that there is an <span class="math inline">\(n - 2\)</span> in the denominator. In other words, as the sample size <span class="math inline">\(n\)</span> increases, the standard error <span class="math inline">\(\text{SE}_{b_1}\)</span> decreases. Just as we demonstrated in Subsection <a href="#moral-of-the-story"><strong>??</strong></a> when we used shovels with <span class="math inline">\(n\)</span> = 25, 50, and 100 slots, the amount of sampling variation of the fitted slope <span class="math inline">\(b_1\)</span> will depend on the sample size <span class="math inline">\(n\)</span>. In particular, as the sample size increases, both the sampling and bootstrap distributions narrow and the standard error <span class="math inline">\(\text{SE}_{b_1}\)</span> decreases. Hence, our estimates of <span class="math inline">\(b_1\)</span> for the true population slope <span class="math inline">\(\beta_1\)</span> get more and more <em>precise</em>.</p>
<p>R then uses this formula for the standard error of <span class="math inline">\(b_1\)</span> in the third column of the regression table and subsequently to construct 95% confidence intervals. But what about the hypothesis test? Much like with our theory-based hypothesis test in Subsection <a href="#theory-hypo"><strong>??</strong></a>, R uses the following <em><span class="math inline">\(t\)</span>-statistic</em> as the test statistic for hypothesis testing:</p>
<p><span class="math display">\[
t = \dfrac{ b_1 - \beta_1}{ \text{SE}_{b_1}}
\]</span></p>
<p>And since the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> is assumed during the hypothesis test, the <span class="math inline">\(t\)</span>-statistic becomes</p>
<p><span class="math display">\[
t = \dfrac{ b_1 - 0}{ \text{SE}_{b_1}} = \dfrac{ b_1 }{ \text{SE}_{b_1}}
\]</span></p>
<p>What are the values of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(\text{SE}_{b_1}\)</span>? They are in the <code>estimate</code> and <code>std_error</code> column of the regression table in Table <a href="1-inference-for-regression.html#tab:regtable-11">1.1</a>. Thus the value of 4.09 in the table is computed as 0.067/0.016 = 4.188. Note there is a difference due to some rounding error here.</p>
<p>Lastly, to compute the <span class="math inline">\(p\)</span>-value, we need to compare the observed test statistic of 4.09 to the appropriate null distribution. Recall from Section <a href="#understanding-ht"><strong>??</strong></a>, that a null distribution is the sampling distribution of the test statistic <em>assuming the null hypothesis <span class="math inline">\(H_0\)</span> is true</em>. Much like in our theory-based hypothesis test in Subsection <a href="#theory-hypo"><strong>??</strong></a>, it can be mathematically proven that this distribution is a <span class="math inline">\(t\)</span>-distribution with degrees of freedom equal to <span class="math inline">\(df = n - 2 = 463 - 2 = 461\)</span>.</p>
<p>Don’t worry if you’re feeling a little overwhelmed at this point. There is a lot of background theory to understand before you can fully make sense of the equations for theory-based methods. That being said, theory-based methods and simulation-based methods for constructing confidence intervals and conducting hypothesis tests often yield consistent results. As mentioned before, in our opinion, two large benefits of simulation-based methods over theory-based are that (1) they are easier for people new to statistical inference to understand, and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist.</p>
</div>
<div id="summary-of-statistical-inference" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Summary of statistical inference</h3>
<p>We’ve finished the last two scenarios from the “Scenarios of sampling for inference” table in Subsection <a href="#sampling-conclusion-table"><strong>??</strong></a>, which we re-display in Table <a href="1-inference-for-regression.html#tab:table-ch11">1.4</a>.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:table-ch11">TABLE 1.4: </span>Scenarios of sampling for inference
</caption>
<thead>
<tr>
<th style="text-align:right;">
Scenario
</th>
<th style="text-align:left;">
Population parameter
</th>
<th style="text-align:left;">
Notation
</th>
<th style="text-align:left;">
Point estimate
</th>
<th style="text-align:left;">
Symbol(s)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;width: 0.5in; ">
1
</td>
<td style="text-align:left;width: 1.5in; ">
Population proportion
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(p\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Sample proportion
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\widehat{p}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
2
</td>
<td style="text-align:left;width: 1.5in; ">
Population mean
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\mu\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Sample mean
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\overline{x}\)</span> or <span class="math inline">\(\widehat{\mu}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
3
</td>
<td style="text-align:left;width: 1.5in; ">
Difference in population proportions
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(p_1 - p_2\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Difference in sample proportions
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\widehat{p}_1 - \widehat{p}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
4
</td>
<td style="text-align:left;width: 1.5in; ">
Difference in population means
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\mu_1 - \mu_2\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Difference in sample means
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\overline{x}_1 - \overline{x}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;width: 0.5in; ">
5
</td>
<td style="text-align:left;width: 1.5in; ">
Population regression slope
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(\beta_1\)</span>
</td>
<td style="text-align:left;width: 1.6in; ">
Fitted regression slope
</td>
<td style="text-align:left;width: 0.65in; ">
<span class="math inline">\(b_1\)</span> or <span class="math inline">\(\widehat{\beta}_1\)</span>
</td>
</tr>
</tbody>
</table>
<p>Armed with the regression modeling techniques you learned in Chapters <a href="#regression"><strong>??</strong></a> and <a href="#multiple-regression"><strong>??</strong></a>, your understanding of sampling for inference in Chapter <a href="#sampling"><strong>??</strong></a>, and the tools for statistical inference like confidence intervals and hypothesis tests in Chapters <a href="#confidence-intervals"><strong>??</strong></a> and <a href="#hypothesis-testing"><strong>??</strong></a>, you’re now equipped to study the significance of relationships between variables in a wide array of data! Many of the ideas presented here can be extended into multiple regression and other more advanced modeling techniques.</p>
</div>
<div id="additional-resources" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Additional resources</h3>
<p>An R script file of all R code used in this chapter is available <a href="scripts/10-inference-for-regression.R">here</a>.</p>
</div>
<div id="whats-to-come" class="section level3">
<h3><span class="header-section-number">1.5.4</span> What’s to come</h3>
<p>You’ve now concluded the last major part of the book on “Statistical Inference with <code>infer</code>.” The closing Chapter <a href="#thinking-with-data"><strong>??</strong></a> concludes this book with various short case studies involving real data, such as house prices in the city of Seattle, Washington in the US. You’ll see how the principles in this book can help you become a great storyteller with data!</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foreword.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/moderndive/moderndive_book/edit/master/10-inference-for-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
